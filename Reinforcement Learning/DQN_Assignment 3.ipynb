{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e70a5b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# import 'gymnasium' and 'minigrid' for our environment\n",
    "import gymnasium as gym\n",
    "import minigrid\n",
    "from minigrid.wrappers import *\n",
    "\n",
    "# import 'random' to generate random numbers\n",
    "import random\n",
    "\n",
    "# import 'numpy' for various mathematical, vector and matrix functions\n",
    "import numpy as np\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "# import 'Pytorch' for all our neural network needs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# if gpu is to be used, otherwise use cpu\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Import 'namedtuple' and 'deque' for Experience Replay Memory\n",
    "from collections import namedtuple, deque\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7fe8e",
   "metadata": {},
   "source": [
    "#### Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6ca8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the gym environment\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "env = ImgObsWrapper(env)\n",
    "env.reset()\n",
    "\n",
    "# extract the object_idx information as a matrix\n",
    "def extractObjectInformation(observation):\n",
    "    (rows, cols, x) = observation.shape\n",
    "    view = np.zeros((rows, cols))\n",
    "    for r in range(rows): \n",
    "        for c in range(cols): \n",
    "            view[r,c] = observation[r,c,0]\n",
    "    return view\n",
    "\n",
    "# the following is a more efficient method of extracting the information using numpy slicing and reshaping\n",
    "def extractObjectInformation2(observation):\n",
    "    (rows, cols, x) = observation.shape\n",
    "    tmp = np.reshape(observation,[rows*cols*x,1], 'F')[0:rows*cols]\n",
    "    return np.reshape(tmp, [rows,cols],'C')\n",
    "\n",
    "# Normalise the input observation so each element is a scalar value between [0,1]\n",
    "def normalize(observation, max_value):\n",
    "    return np.array(observation)/max_value\n",
    "\n",
    "# Flatten the [7,7] observation matrix into a [1,49] tensor\n",
    "def flatten(observation):\n",
    "    return torch.from_numpy(np.array(observation).flatten()).float().unsqueeze(0)\n",
    "\n",
    "# Combine all the preprocessing fuctions into a single function\n",
    "def preprocess(observation):\n",
    "    return flatten(normalize(extractObjectInformation2(observation), 10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27328361",
   "metadata": {},
   "source": [
    "#### Setting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88264614",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS \n",
    "numActions = 3               # 3 possible actions: left, right, move forward\n",
    "inputSize = 49               # size of the flattened input state (7x7 matrix of tile IDs)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "alpha = 0.0002               # learning_rate\n",
    "episodes = 5000              # Total episodes for training\n",
    "batch_size = 128             # Neural network batch size\n",
    "target_update = 20000        # Number of episodes between updating target network\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.90                 # Discounting rate\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "start_epsilon = 1.0          # exploration probability at start\n",
    "stop_epsilon = 0.01          # minimum exploration probability \n",
    "decay_rate = 100           # exponential decay rate for exploration prob\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size # Number of experiences stored in the Memory when initialized for the first time\n",
    "memorySize = 500000          # Number of experiences the Memory can keep - 500000\n",
    "\n",
    "### TESTING HYPERPARAMETERS\n",
    "# Evaluation hyperparameter\n",
    "evalEpisodes = 1000          # Number of episodes to be used for evaluation\n",
    "\n",
    "# Change this to 'False' if you only want to evaluate a previously trained agent\n",
    "train = True     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad203a",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7458e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc1): Linear(in_features=49, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Neural network model definition\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputSize, numActions, hiddenLayerSize=(512, 256)):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, hiddenLayerSize[0])\n",
    "        self.fc2 = nn.Linear(hiddenLayerSize[0], hiddenLayerSize[1])\n",
    "        self.fc3 = nn.Linear(hiddenLayerSize[1], numActions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# Instantiate the policy network and the target network\n",
    "\n",
    "hiddenLayerSize = (128,128)\n",
    "policy_net = DQN(inputSize, numActions, hiddenLayerSize)\n",
    "target_net = DQN(inputSize, numActions, hiddenLayerSize)\n",
    "\n",
    "# Copy the weights of the policy network to the target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# We don't want to update\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ba20e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_values:  tensor([[-0.0493,  0.0913, -0.0486]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "best action:  torch.return_types.max(\n",
      "values=tensor([0.0913], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([1]))\n",
      "\n",
      "a:  tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# For this example we first need to get an observation by resetting the environment\n",
    "obs, _ =  env.reset()\n",
    "\n",
    "# We then preprocess the observation to obtain our state\n",
    "state = preprocess(obs)\n",
    "\n",
    "# Lastly we apply the state as input to our policy network\n",
    "action_values = policy_net(state)\n",
    "\n",
    "print('action_values: ',action_values)\n",
    "\n",
    "# If want want to get the action that has the highest Q-value we use the 'max' function. \n",
    "# The result is a tuple where the first element is the value, and the second element is the index\n",
    "action_values.max(1)\n",
    "print('\\nbest action: ', action_values.max(1))\n",
    "a = action_values.max(1)[1]\n",
    "print('\\na: ', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53ccbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to e-greedily select next action based on current state\n",
    "def select_action(state):\n",
    "    # generate a random number\n",
    "    sample = random.random()\n",
    "    \n",
    "    # calculate the epsilon threshold, based on the epsilon-start value, the epsilon-stop value, \n",
    "    # the number of training steps taken and the epsilon decay rate\n",
    "    # here we are using an exponential decay rate for the epsilon value\n",
    "    eps_threshold = stop_epsilon+(start_epsilon-stop_epsilon)*math.exp(-1. * steps_done / decay_rate)\n",
    "    \n",
    "    # compare the generated random number to the epsilon threshold\n",
    "    if sample > eps_threshold:\n",
    "        # act greedily towards the Q-values of our policy network, given the state\n",
    "        \n",
    "        # we do not want to gather gradients as we are only generating experience, not training the network\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].unsqueeze(0)\n",
    "    else:\n",
    "        # select a random action with equal probability\n",
    "        return torch.tensor([[random.randrange(numActions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60495faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('currentState', 'action', 'nextState', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "# Instantiate memory\n",
    "memory = ReplayMemory(memorySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf4b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "039ba28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83dfc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training of the model\n",
    "batch_size = 2\n",
    "def optimize_model():\n",
    "\n",
    "    # check if the replay memory has stored enough experience\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    # Sample mini-batch\n",
    "    experience = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*experience))\n",
    "    \n",
    "    # Calculate action-values using policy network\n",
    "    state_batch = torch.cat(batch.currentState)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Calculate TD-targets using target network\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    non_final_next_states = torch.cat([s for s in batch.nextState\n",
    "                                                if s is not None])\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.nextState)), device=device, dtype=torch.bool)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    TDtargets = (next_state_values * gamma) + reward_batch\n",
    "    TDerrors = TDtargets.unsqueeze(1) - state_action_values\n",
    "    \n",
    "    # Calculate loss\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, TDtargets.unsqueeze(1))\n",
    "    \n",
    "    # Make gradient descrent step and update policy network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9a2555f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9c5875ac4f60c987\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9c5875ac4f60c987\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75d0df1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Finished episode successfully taking 218 steps and receiving reward 0.233594\n",
      "Truncated episode taking 256 steps and receiving reward 0.000000\n",
      "Finished episode successfully taking 127 steps and receiving reward 0.553516\n",
      "Finished episode successfully taking 45 steps and receiving reward 0.841797\n",
      "Truncated episode taking 256 steps and receiving reward 0.000000\n",
      "Finished episode successfully taking 203 steps and receiving reward 0.286328\n",
      "Truncated episode taking 256 steps and receiving reward 0.000000\n",
      "Finished episode successfully taking 170 steps and receiving reward 0.402344\n",
      "Finished episode successfully taking 96 steps and receiving reward 0.662500\n",
      "Truncated episode taking 256 steps and receiving reward 0.000000\n",
      "Done training...\n"
     ]
    }
   ],
   "source": [
    "# Make the gym environment\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "\n",
    "# Use a wrapper so the observation only contains the grid information\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "episodes = 10               # total number of training episodes\n",
    "max_steps = env.max_steps  # maximum number of steps allowed before truncating episode\n",
    "steps_done = 0             # total training steps taken\n",
    "\n",
    "memory = ReplayMemory(memorySize) # Instantiate memory\n",
    "batch_size = 2\n",
    "total_rewards = 0\n",
    "\n",
    "print('Start training...')\n",
    "for e in range(episodes):\n",
    "    \n",
    "    # reset the environment\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # extract the current state from the observation\n",
    "    state = preprocess(obs)\n",
    "    \n",
    "    for i in range(0, max_steps):\n",
    "\n",
    "        # Choose an action\n",
    "        # Pick a random action\n",
    "        action = select_action(state)\n",
    "        a = action.item()\n",
    "        \n",
    "        # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "        # 'done' indicate if the termination state was reached\n",
    "        obs, reward_, done, truncated, info = env.step(a)\n",
    "        reward = torch.tensor([reward_], device = device)\n",
    "   \n",
    "        # extract the next state from the observation\n",
    "        nextState = preprocess(obs)\n",
    "        \n",
    "        # if the episode is finished, the nextState is set to None to indicate that the\n",
    "        # <s,a,r,s'> transition led to a terminating state\n",
    "        if (done or truncated):\n",
    "            nextState = None\n",
    "        \n",
    "        # Store the transition <s,a,r,s'> in the replay memory\n",
    "        memory.push(state, action, nextState, reward)\n",
    "\n",
    "        # Move to the next state          \n",
    "        currentState = nextState\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network) by\n",
    "        # sample a mini-batch and train the model using the sampled mini-batch\n",
    "        optimize_model()\n",
    "        \n",
    "        # If the target update threshold is reached, update the target network, \n",
    "        # copying all weights and biases in the policy network   \n",
    "        if steps_done % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        \n",
    "        # Episode finished when done or truncated is true\n",
    "        if (done or truncated):\n",
    "            # Record the reward and total training steps taken\n",
    "            if (done):\n",
    "                # if agent reached its goal successfully\n",
    "                print('Finished episode successfully taking %d steps and receiving reward %f' % (env.step_count, reward))\n",
    "            else:\n",
    "                # agent failed to reach its goal successfully \n",
    "                print('Truncated episode taking %d steps and receiving reward %f' % (env.step_count, reward))\n",
    "            break\n",
    "            \n",
    "    steps_done += 1\n",
    "    total_rewards += reward_\n",
    "    writer.add_scalar(\"Reward/train\", total_rewards/(e+1), e)\n",
    "            \n",
    "        \n",
    "print('Done training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b7b0668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed evaluation episode 0 with reward 0.000000, 256 steps\n",
      "Failed evaluation episode 1 with reward 0.000000, 256 steps\n",
      "Failed evaluation episode 2 with reward 0.000000, 256 steps\n",
      "Failed evaluation episode 3 with reward 0.000000, 256 steps\n",
      "Failed evaluation episode 4 with reward 0.000000, 256 steps\n",
      "Failed evaluation episode 5 with reward 0.000000, 256 steps\n",
      "Failed evaluation episode 6 with reward 0.000000, 256 steps\n",
      "Failed evaluation episode 7 with reward 0.000000, 256 steps\n",
      "Failed evaluation episode 8 with reward 0.000000, 256 steps\n",
      "Failed evaluation episode 9 with reward 0.000000, 256 steps\n",
      "Completion rate 0.00 with average reward 0.0000 and average steps 1280.00\n"
     ]
    }
   ],
   "source": [
    "# evaluation loop\n",
    "finishCounter = 0.0\n",
    "totalSteps = 0.0\n",
    "totalReward = 0.0\n",
    "\n",
    "steps_done = 1000000\n",
    "stop_epsilon = 0.0\n",
    "evalEpisodes = 2\n",
    "\n",
    "for e in range(evalEpisodes):\n",
    "    # Initialize the environment and state\n",
    "    currentObs, _ = env.reset()\n",
    "    currentState = preprocess(currentObs)\n",
    "   \n",
    "    # the main RL loop\n",
    "    for i in range(0, env.max_steps):\n",
    "        # Select and perform an action\n",
    "        action = select_action(currentState)\n",
    "        a = action.item()\n",
    "\n",
    "        # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "        # 'done' indicate if the termination state was reached\n",
    "        obs, reward, done, truncated, info = env.step(a)\n",
    "        \n",
    "        if (done or truncated):\n",
    "            # Observe new state\n",
    "            nextState = None\n",
    "        else:\n",
    "            nextState = preprocess(obs)\n",
    "\n",
    "        if (done or truncated):\n",
    "            totalReward += reward\n",
    "            totalSteps += env.step_count\n",
    "            if (done):\n",
    "                print('Finished evaluation episode %d with reward %f,  %d steps, reaching goal ' % (e, reward, env.step_count))\n",
    "                finishCounter += 1\n",
    "            if (truncated):\n",
    "                print('Failed evaluation episode %d with reward %f, %d steps' % (e,reward, env.step_count))\n",
    "            break\n",
    "        \n",
    "        # Move to the next state\n",
    "        currentState = nextState\n",
    "\n",
    "# Print a summary of the evaluation results\n",
    "print('Completion rate %.2f with average reward %0.4f and average steps %0.2f' % (finishCounter/evalEpisodes, totalReward/evalEpisodes,  totalSteps/evalEpisodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb5501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
